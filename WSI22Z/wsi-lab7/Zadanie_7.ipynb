{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "# Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa. \n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt** \n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "np.seterr(divide = 'ignore')\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        # entries in likelihood are matrices, where rows are 4-long arrays of likelihoods of each bin\n",
        "        self.likelihoods = {}\n",
        "        self.binCount = 4\n",
        "        self.bins = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "\n",
        "        # likelihoods - probability that a feature has given value when we know class is `i`.\n",
        "        # Calculated from normal distribution\n",
        "        normalise = lambda v : v/len(train_classes)\n",
        "        counts = Counter(train_classes)\n",
        "        self.priors = {k: normalise(v) for k, v in counts.items()}\n",
        "\n",
        "\n",
        "        discrete_features = self.data_discretization(train_features, self.binCount)\n",
        "\n",
        "        # for every class\n",
        "        for c in counts.keys():\n",
        "            # get rows with matching class\n",
        "            matching = np.array([row for idx,row in enumerate(discrete_features) if train_classes[idx]==c])\n",
        "            # find ratio between all and found\n",
        "            normalise = lambda v : v/len(matching)\n",
        "            x = {}\n",
        "            # do this for all columns\n",
        "            for i,col in enumerate(matching.T):\n",
        "                # count how many times this feature appeared\n",
        "                featureCounts = Counter(col)\n",
        "                x[i] = {k: normalise(v) for k, v in featureCounts.items()}\n",
        "                # add zeros if feature didnt show up\n",
        "                for bin in range(1,self.binCount+1):\n",
        "                    if bin not in col:\n",
        "                        x[i][bin]=0\n",
        "            \n",
        "            self.likelihoods[c]=x # for this class, here's a likehood dict\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "    def calculateBins(self, whole_data):\n",
        "        for i,col in enumerate(whole_data.T):\n",
        "            cmin,cmax = col.min(),col.max()\n",
        "            self.bins[i] = np.linspace(cmin, cmax, self.binCount+1)\n",
        "        return self\n",
        "\n",
        "    #@staticmethod\n",
        "    def data_discretization(self,data, segment_count):\n",
        "        digitized_arr = np.zeros_like(data,dtype=np.int8)\n",
        "        for i in range(data.shape[1]):\n",
        "            column = data[:, i]\n",
        "            digitized_arr[:, i] = np.digitize(column, self.bins[i], right=True)\n",
        "        return digitized_arr\n",
        "\n",
        "\n",
        "    def predict(self, sample):\n",
        "        # calculate probability of each class, return most probable one\n",
        "        # the idea is that we assume that features are independent, so \n",
        "        # the likehood of given features vector in some class is\n",
        "        # product of likehoods of singular features \n",
        "        predictions = {}\n",
        "        discreteSample = [np.digitize(feature, self.bins[i]) for i,feature in enumerate(sample)]\n",
        "        for cls in self.priors:\n",
        "            #P(cechy|cls)=PI(cecha_i|cls)\n",
        "            likelihood=1\n",
        "            for i,value in enumerate(discreteSample):\n",
        "                likelihood*=self.likelihoods[cls][i][value]\n",
        "            predictions[cls]=likelihood*self.priors[cls]\n",
        "        \n",
        "        return max(predictions, key=predictions.get) \n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        #self.likelihoods = {}\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        # priors - probability of class in whole dataset\n",
        "        # priors[i] = p(klasa=i) dla i € klasy\n",
        "        normalise = lambda v : v/len(train_classes)\n",
        "        counts = Counter(train_classes)\n",
        "        self.priors = {k: normalise(v) for k, v in counts.items()}\n",
        "\n",
        "        # calculate mean and std of each feature for each class\n",
        "        # then likelihood is return of normal distribution density with these params at given point\n",
        "        for c in counts.keys():\n",
        "            matching = np.array([row for idx,row in enumerate(train_features) if train_classes[idx]==c])\n",
        "            self.means[c]=np.array([np.mean(column) for column in matching.T])\n",
        "            self.stds[c]=np.array([np.std(column) for column in matching.T])\n",
        "\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_dist(x: np.ndarray, mean: np.ndarray, std: np.ndarray):\n",
        "        return 1/(np.sqrt(2*np.pi)*std)*np.exp(-np.power((x - mean)/std, 2)/2)\n",
        "\n",
        "    def predict(self, sample):\n",
        "        # like previous, but now we estimate likehoods of the features based on\n",
        "        # mean and std\n",
        "        #           ->      ->                      -> \n",
        "        # P(klasa|cechy)=P(cechy|klasa)*P(klasa)/P(cechy)\n",
        "        #   * dla każdej klasy liczymy P, wybieramy klase z max wynikiem\n",
        "        #   * P(cechy) ma jakąś wartośc, ale dla wybrania maksymalnego wyniku można pominąć,\n",
        "        #     bo i tak wyniki będą proporcjonalne             -> \n",
        "        #   * P(cechy|klasa) liczymy właśnie z gaussa, ale P(cechy|klasa)=P(cecha1|klasa)*P(cecha2|klasa)*...\n",
        "        #     bo zakładamy że niezależne\n",
        "        predictions = {}\n",
        "        for cls in self.priors:\n",
        "            likelihood = np.prod(self.normal_dist(sample,self.means[cls],self.stds[cls]))\n",
        "            predictions[cls]=likelihood*self.priors[cls]\n",
        "        \n",
        "        return max(predictions, key=predictions.get) \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seed 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian accuracy: 15/15 100.0%\n",
            "Normal accuracy: 14/15 93.33333333333333%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "normal = NaiveBayes().calculateBins(x).build_classifier(x_train,y_train)\n",
        "gaussian = GaussianNaiveBayes().build_classifier(x_train,y_train)\n",
        "\n",
        "gaussianHits,normalHits = 0,0\n",
        "\n",
        "for x,y in zip(x_test,y_test):\n",
        "    gaussianHits+=gaussian.predict(x)==y\n",
        "    normalHits+=normal.predict(x)==y\n",
        "\n",
        "print(f\"Gaussian accuracy: {gaussianHits}/{len(y_test)} {100*gaussianHits/len(y_test)}%\")\n",
        "print(f\"Normal accuracy: {normalHits}/{len(y_test)} {100*normalHits/len(y_test)}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "split: 0.1\n",
            "Gaussian accuracy: 14287/15000 95.24666666666667%\n",
            "Normal accuracy: 12638/15000 84.25333333333333%\n",
            "split: 0.3\n",
            "Gaussian accuracy: 42886/45000 95.30222222222223%\n",
            "Normal accuracy: 38164/45000 84.80888888888889%\n",
            "split: 0.5\n",
            "Gaussian accuracy: 71382/75000 95.176%\n",
            "Normal accuracy: 63467/75000 84.62266666666666%\n",
            "split: 0.7\n",
            "Gaussian accuracy: 99469/105000 94.73238095238095%\n",
            "Normal accuracy: 87285/105000 83.12857142857143%\n"
          ]
        }
      ],
      "source": [
        "def RunAverageTests(testCount,datax,datay):\n",
        "    for split in (0.1,0.3,0.5,0.7):\n",
        "        gaussianHits,normalHits = 0,0\n",
        "        for i in range(testCount):\n",
        "            x_train, x_test, y_train, y_test = train_test_split(datax, datay, test_size=split)\n",
        "            normal = NaiveBayes().calculateBins(datax).build_classifier(x_train,y_train)\n",
        "            gaussian = GaussianNaiveBayes().build_classifier(x_train,y_train)\n",
        "\n",
        "            for x,y in zip(x_test,y_test):\n",
        "                #print(gaussian.predict(x),y)\n",
        "                gaussianHits+=gaussian.predict(x)==y\n",
        "                normalHits+=normal.predict(x)==y\n",
        "        print(\"split:\",split)\n",
        "        print(f\"Gaussian accuracy: {gaussianHits}/{len(y_test)*testCount} {100*gaussianHits/(len(y_test)*testCount)}%\")\n",
        "        print(f\"Normal accuracy: {normalHits}/{len(y_test)*testCount} {100*normalHits/(len(y_test)*testCount)}%\")\n",
        "    \n",
        "\n",
        "iris = load_iris()\n",
        "RunAverageTests(1000,iris.data,iris.target)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wnioski\n",
        "\n",
        "Klasyfikator \"niezależny\" nosi swoją nazwę z powodu, że zakładamy niezależność zmiennych (cech), przez co policzenie prawdopodobnści oznacza policzenie iloczynu pojedynczych prawdopodobności.\n",
        "\n",
        "\n",
        "Wyniki dla `random_seed=123` są 15/15 dla Gaussa oraz 14/15 dla dyskretyzatora:\n",
        "```\n",
        "Gaussian accuracy: 15/15 100.0%\n",
        "Normal accuracy: 14/15 93.33333333333333%\n",
        "```\n",
        "\n",
        "Średnia celnośc obydwu modeli z 1000 uruchomień::\n",
        "* podział zbioru 0.1\n",
        "    * Gaussian: 14280/15000 95.2%\n",
        "    * Dyskretyzator: 12638/15000 84.25%\n",
        "* podział zbioru 0.3\n",
        "    * Gaussian: 42932/45000 95.40%\n",
        "    * Dyskretyzator: 38164/45000 84.80%\n",
        "* podział zbioru 0.5\n",
        "    * Gaussian: 71452/75000 95.27%\n",
        "    * Dyskretyzator: 63467/75000 84.62%\n",
        "* podział zboiru 0.7\n",
        "    * Gaussian: 99367/105000 94.64%\n",
        "    * Dyskretyzator: 87285/105000 83.12%\n",
        "\n",
        "\n",
        "Podejście z dyskretyzowaniem zmiennych na 4 \"kubełki\" wypada trochę gorzej od gaussa.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
