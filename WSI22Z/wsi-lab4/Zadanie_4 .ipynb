{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "# Zadanie 4 (7 pkt)\n",
        "\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    '''\n",
        "    ratio between occurences of class in whole set\n",
        "    '''\n",
        "    freq = class_count/num_samples\n",
        "    return freq*math.log(freq)\n",
        "\n",
        "\n",
        "# used to calculate InfGain -> make groups based on atribute,\n",
        "# find their entropy, easy to use\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        # array of classes in current tree part [0,1,2,1,2,0,...]\n",
        "        self.group_classes = group_classes\n",
        "        self.entropy = self.group_entropy()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.group_classes.size\n",
        "\n",
        "    def group_entropy(self):\n",
        "        s = 0.0\n",
        "        unique = set(self.group_classes)\n",
        "        for cl in unique:\n",
        "            howmany = np.count_nonzero(self.group_classes == cl)\n",
        "            s -= entropy_func(howmany, len(self.group_classes))\n",
        "        return s\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, depth=None, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature  # feature used to split\n",
        "        self.split_val = split_val\n",
        "        self.depth = depth\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        node = self\n",
        "        while node.val is None:\n",
        "            f, v = node.split_feature, node.split_val\n",
        "            data_value = data[f]\n",
        "            if data_value < v:\n",
        "                node = node.child_node_a\n",
        "            else:\n",
        "                node = node.child_node_b\n",
        "        return node.val\n",
        "\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth):\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    # group_a is the subset of group_b\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        return (len(group_a)/len(group_b))*group_a.entropy\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        parentGroup = Group(parent_group)\n",
        "        splitent = sum(self.get_split_entropy(Group(child), parentGroup)\n",
        "                       for child in [child_group_a, child_group_b])\n",
        "        return parentGroup.entropy-splitent\n",
        "\n",
        "    def get_best_split(self, data, classes, which_feature_to_use):\n",
        "        # go over all possible values...\n",
        "        # first step is to get them\n",
        "        possible_values = set(data[:,which_feature_to_use])\n",
        "        best_split = None\n",
        "        best_gain = 0\n",
        "        for point in possible_values:\n",
        "            # get corresponding classes here instead of values\n",
        "            a = [classes[idx] for idx, row in enumerate(\n",
        "                data) if row[which_feature_to_use] >= point]\n",
        "            first = np.asarray(a)\n",
        "\n",
        "            second = np.asarray([classes[idx] for idx, row in enumerate(\n",
        "                data) if row[which_feature_to_use] < point])\n",
        "            gain = self.get_information_gain(classes, first, second)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_split = point\n",
        "        return best_split, best_gain\n",
        "\n",
        "    def get_best_feature_split(self, feature_values, classes):\n",
        "        best_feature = 0\n",
        "        best_split = 0\n",
        "        best_gain = 0\n",
        "        for feature in range(len(feature_values[0])):\n",
        "            split, gain = self.get_best_split(feature_values, classes, feature)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_split = split\n",
        "                best_feature = feature\n",
        "        return best_feature, best_split\n",
        "\n",
        "    def split_data_and_classes(self, data, classes, feature, value):\n",
        "        '''\n",
        "        returns two new children subsets divided by feature at value,\n",
        "        feature is feature index (column number in dataset)\n",
        "        '''\n",
        "        classes_GE = [classes[idx] for idx, row in enumerate(\n",
        "            data) if row[feature] >= value]\n",
        "        classes_L = [classes[idx] for idx, row in enumerate(\n",
        "            data) if row[feature] < value]\n",
        "        data_GE = [data[idx] for idx, row in enumerate(\n",
        "            data) if row[feature] >= value]\n",
        "        data_L = [data[idx] for idx, row in enumerate(\n",
        "            data) if row[feature] < value]\n",
        "        return (data_L, classes_L), (data_GE, classes_GE)\n",
        "\n",
        "    def build_tree(self, data, classes, depth=0):\n",
        "        self.tree = self._build_tree_rec(data, classes, depth)\n",
        "\n",
        "    def _build_tree_rec(self, data, classes, depth):\n",
        "        ''' data - attributes\n",
        "            classes - target class\n",
        "\n",
        "            here use Node\n",
        "        '''\n",
        "        # albo depth wyczerpany, albo pozostała jedna klasa\n",
        "        if depth == self.max_depth or len(set(classes)) == 1:\n",
        "\n",
        "            # nie możemy dalej dzielić, patrzymy która klasa w tym momencie\n",
        "            # jest najliczniejsza, ona będzie najprawdopodobniejsza\n",
        "            # (lub jest jedyna)\n",
        "            bestClass = np.argmax(np.bincount(classes))\n",
        "            return Node(None, None, 0, None, None, bestClass)\n",
        "\n",
        "        else:\n",
        "            # patrzymy w jaki sposób najbardziej opłaca się podzielić\n",
        "            best_feature, best_split = self.get_best_feature_split(\n",
        "                data, classes)\n",
        "\n",
        "            #  2 tuples (data, classes)\n",
        "            partA, partB = self.split_data_and_classes(\n",
        "                data, classes, best_feature, best_split)\n",
        "\n",
        "            childA = self._build_tree_rec(np.asarray(\n",
        "                partA[0]), np.asarray(partA[1]), depth+1)\n",
        "            childB = self._build_tree_rec(np.asarray(\n",
        "                partB[0]), np.asarray(partB[1]), depth+1)\n",
        "\n",
        "            return Node(best_feature, best_split,\n",
        "                        depth, childA, childB, None)\n",
        "    \n",
        "    def printTree(self, node: Node, indent=0):\n",
        "        tab = \"\\t\"*indent\n",
        "        if node.val is not None:\n",
        "            print(f'{tab}class:{node.val}')\n",
        "        else:\n",
        "            print(f'{tab}{node.split_val}, {node.split_feature}')\n",
        "        if node.child_node_a:\n",
        "            self.printTree(node.child_node_a,indent+1)\n",
        "        if node.child_node_b:\n",
        "            self.printTree(node.child_node_b,indent+1)\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* test_size: 0.1\n",
            "    * depth 1: 0.6\n",
            "    * depth 2: 0.9066666666666666\n",
            "    * depth 3: 0.92\n",
            "    * depth 15: 0.9866666666666667\n",
            "* test_size: 0.2\n",
            "    * depth 1: 0.6266666666666667\n",
            "    * depth 2: 0.92\n",
            "    * depth 3: 0.92\n",
            "    * depth 15: 0.9666666666666667\n",
            "* test_size: 0.3\n",
            "    * depth 1: 0.6444444444444445\n",
            "    * depth 2: 0.9111111111111111\n",
            "    * depth 3: 0.9422222222222222\n",
            "    * depth 15: 0.9644444444444444\n",
            "* test_size: 0.4\n",
            "    * depth 1: 0.68\n",
            "    * depth 2: 0.9066666666666666\n",
            "    * depth 3: 0.93\n",
            "    * depth 15: 0.9466666666666667\n",
            "* test_size: 0.5\n",
            "    * depth 1: 0.5946666666666667\n",
            "    * depth 2: 0.9386666666666666\n",
            "    * depth 3: 0.9386666666666666\n",
            "    * depth 15: 0.9466666666666667\n",
            "* test_size: 0.6\n",
            "    * depth 1: 0.6377777777777778\n",
            "    * depth 2: 0.9266666666666666\n",
            "    * depth 3: 0.9333333333333333\n",
            "    * depth 15: 0.9244444444444444\n",
            "* test_size: 0.7\n",
            "    * depth 1: 0.6323809523809524\n",
            "    * depth 2: 0.9066666666666666\n",
            "    * depth 3: 0.9219047619047619\n",
            "    * depth 15: 0.9314285714285714\n",
            "3.0, 2\n",
            "\tclass:0\n",
            "\t4.9, 2\n",
            "\t\t1.7, 3\n",
            "\t\t\tclass:1\n",
            "\t\t\tclass:2\n",
            "\t\t1.8, 3\n",
            "\t\t\tclass:2\n",
            "\t\t\tclass:2\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "# driver code\n",
        "testcount=5 \n",
        "lasttestsize=-1\n",
        "for tsize,depth in itertools.product([0.1,0.2,0.3,0.4,0.5,0.6,0.7],[1,2,3,15]):\n",
        "    if lasttestsize!=tsize:\n",
        "        print(\"* test_size:\",tsize)\n",
        "        lasttestsize=tsize\n",
        "    good_sum=0\n",
        "    for test in range(testcount):\n",
        "        # x_train -> y_train, próbki z całego zbioru do nauki, x wyznacza y\n",
        "        # x_test -> y_test, pozostałe próbki do przetestowania\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=tsize)\n",
        "        good_counter = 0\n",
        "        dc = DecisionTreeClassifier(depth)\n",
        "        dc.build_tree(x_train, y_train)\n",
        "        for sample, gt in zip(x_test, y_test):\n",
        "            prediction = dc.predict(sample)\n",
        "            # print(f\"prediction/real:{prediction}/{gt}\")\n",
        "            if prediction == gt:\n",
        "                good_counter += 1\n",
        "        percent = good_counter/len(y_test)*100\n",
        "        # print(f\"Accuracy: {good_counter}/{len(y_test)} ({percent:.2f}%)\")\n",
        "        good_sum+=good_counter\n",
        "    print(f\"    * depth {depth}: {good_sum/(len(y_test)*testcount)}\")\n",
        "\n",
        "# statyczny seed\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1,random_state=123)\n",
        "\n",
        "dc = DecisionTreeClassifier(3)\n",
        "dc.build_tree(x_train, y_train)\n",
        "\n",
        "dc.printTree(dc.tree)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wnioski\n",
        "\n",
        "Średnia dokładność dla różnych głębokości i różnych rozmiarów danych\n",
        "\n",
        "* test_size 0.1\n",
        "\t* depth 1: 0.6\n",
        "\t* depth 2: 0.9466666666666667\n",
        "\t* depth 3: 0.9466666666666667\n",
        "\t* depth 15: 0.92\n",
        "\n",
        "* test_size 0.2\n",
        "\t* depth 1: 0.6333333333333333\n",
        "\t* depth 2: 0.9266666666666666\n",
        "\t* depth 3: 0.9133333333333333\n",
        "\t* depth 15: 0.92\n",
        "\n",
        "* test_size 0.3\n",
        "\t* depth 1: 0.6311111111111111\n",
        "\t* depth 2: 0.8888888888888888\n",
        "\t* depth 3: 0.9111111111111111\n",
        "\t* depth 15: 0.9377777777777778\n",
        "\n",
        "* test_size 0.4\n",
        "\t* depth 1: 0.6366666666666667\n",
        "\t* depth 2: 0.9233333333333333\n",
        "\t* depth 3: 0.93\n",
        "\t* depth 15: 0.93\n",
        "\n",
        "* test_size 0.5\n",
        "\t* depth 1: 0.6373333333333333\n",
        "\t* depth 2: 0.9253333333333333\n",
        "\t* depth 3: 0.936\n",
        "\t* depth 15: 0.9466666666666667\n",
        "\n",
        "* test_size 0.6\n",
        "\t* depth 1: 0.6177777777777778\n",
        "\t* depth 2: 0.9288888888888889\n",
        "\t* depth 3: 0.9266666666666666\n",
        "\t* depth 15: 0.9488888888888889\n",
        "\n",
        "* test_size 0.7\n",
        "\t* depth 1: 0.6323809523809524\n",
        "\t* depth 2: 0.9352380952380952\n",
        "\t* depth 3: 0.940952380952381\n",
        "\t* depth 15: 0.9314285714285714"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
